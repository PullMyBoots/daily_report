# AI 研究的范式革命：从“更大”到“更聪明”的 2025 年终总结

**发布日期**: 2026年1月7日（星期三）  
**创作者**: Manus AI  

---

## 导语：智能的进化，不再是“大力出奇迹”

2025 年，人工智能的进化轨迹发生了一次深刻的转向。如果说过去几年 AI 发展的核心是“更大”，即通过不断增加模型参数和训练数据来提升能力的“暴力美学”，那么 2025 年的主题则转向了“更聪明”。研究界的焦点，从单纯依赖训练阶段的参数扩展，更多地转移到了优化“思考”过程本身——即通过创新的训练方法和推理时的计算策略，让模型在解决问题时展现出更高层次的智能。

这一年，我们见证了 AI 从一个被动的“知识容器”向一个主动的“思考伙伴”的演变。其核心驱动力不再仅仅是预训练阶段的 Scaling Law，而是以 **强化学习**、**推理时扩展** 和 **多模态智能体** 为代表的三大技术浪潮。本文将深入剖析这些在 2025 年至 2026 年初取得关键突破的研究成果，帮助读者理解 AI 能力演进的真实推动力，以及这些底层进展在中长期内，可能为科研与研发工作带来的深刻变革。

---

![AI 研究范式的演进](images/paradigm_shift.png)
*图1: AI 研究范式从预训练扩展到推理时扩展的演进历程*

---

## 1. 推理的黎明：RLVR 与 DeepSeek-R1 引发的"思考"革命

长期以来，大型语言模型（LLM）虽然在文本生成方面表现出色，但在需要严谨逻辑和多步推理的任务上却常常“失足”。2025 年的第一个重大突破，正是对这一核心问题的正面回应，其关键技术便是 **可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）** [1]。

与依赖人类主观偏好进行对齐的 RLHF（基于人类反馈的强化学习）不同，RLVR 采用了一种更为客观和严谨的训练范式。它通过为模型的输出提供一个可被程序自动验证的“奖励信号”——例如，一个数学问题的最终答案是否正确，或者一段代码是否能通过单元测试——来引导模型学习如何进行正确的、可分解的逐步推理。这种方法尤其适用于那些存在明确“正确”与“错误”之分的领域。

> “强化学习比我认为的普通人想象的要糟糕得多。强化学习很糟糕。只是碰巧我们之前拥有的一切都更糟糕。”
> 
> —— Andrej Karpathy [2]

正如 AI 领域知名学者 Andrej Karpathy 所言，尽管强化学习本身存在诸多挑战，但它依然是推动模型能力边界最有效的方法之一。RLVR 的出现，正是这一思想的完美体现。

引爆这一趋势的，是深度求索（DeepSeek）于 2025 年 1 月发布的 **DeepSeek-R1** 模型 [3]。它不仅是第一个在推理能力上公开叫板 OpenAI 顶级模型的开源模型，更重要的是，它向世界展示了其完整的"思考过程"（Chain-of-Thought）。用户第一次可以清晰地看到一个开源模型是如何通过详细的推理链条，一步步解决复杂问题的。这一里程碑式的发布，标志着"推理模型"元年的开启，证明了通过 RLVR 等先进的强化学习技术，我们不仅能让模型"知其然"，更能让它"知其所以然"。

![RLVR 机制图](images/rlvr_mechanism.png)
*图2: RLVR（可验证奖励的强化学习）工作机制示意图*

| 技术范式 | 核心机制 | 优点 | 缺点 |
| :--- | :--- | :--- | :--- |
| **RLHF** | 依赖人类对模型输出的主观偏好进行排序和打分 | 能够对齐人类价值观，提升模型的“情商” | 成本高、速度慢、标注者偏见可能影响结果 |
| **RLAIF** | 使用更强大的 AI 模型作为“老师”来评估和打分 | 速度快、成本低、可大规模扩展 | 容易出现“自我循环”的偏见放大问题 |
| **RLVR** | 使用可被程序自动验证的客观奖励信号（如单元测试） | 奖励信号客观、可靠，特别适合推理任务 | 适用范围有限，主要用于有明确对错标准的领域 |

---

## 2. 智能的新维度：超越参数的“推理时扩展”

如果说 RLVR 是从“训练方法”上提升了模型的内在推理能力，那么 2025 年的另一大研究热点——**推理时扩展（Inference-Time Scaling）**，则是在“使用方法”上开辟了智能的新维度。

这一理念的核心思想是：模型的智能水平并非一个在训练结束后就完全固定的静态值，而是可以通过在回答问题时投入更多计算资源来动态提升的。简单来说，就是允许模型“多想一会儿”，从而得到更准确、更可靠的答案。这一转变，标志着 AI 研究从关注“模型有多大”转向了“模型如何思考”。

2025 年，来自 OpenAI 的一篇被高度引用的论文明确指出，在许多推理任务上，**优化推理时的计算分配，比单纯增加模型参数更有效** [4]。这意味着，即使是同一个模型，通过不同的推理策略，其表现出的“智商”也可能天差地别。这一发现催生了多种推理时扩展技术：

- **多路径推理与验证**：模型生成多个不同的解题思路（思维链），然后通过内部的验证器或投票机制，选出最可靠的一个。这类似于人类解决难题时的“集思广益”和“反复检查”。
- **迭代自我修正**：模型生成初步答案后，会对其进行反思和批判，识别出潜在的错误，然后进行主动修正，多次迭代后得到最终答案。
- **并行协调推理（PaCoRe）**：这种新框架将推理的驱动力从传统的“顺序深度”（一步接一步）转向了“并行广度”，让模型可以同时探索多个推理路径，再进行协调与整合 [5]。

当然，这种"慢思考"并非没有代价。它直接导致了更高的计算成本和更长的响应延迟。因此，推理时扩展的本质，是在 **准确性、成本和速度** 这三者之间进行权衡。对于科研、医疗诊断、法律分析等对准确性要求极高的领域，花费更多时间和算力来换取一个更可靠的结果，无疑是值得的。展望 2026 年，如何根据任务的重要性动态调整计算投入，将成为 AI Agent 设计的核心挑战之一。

![推理时扩展对比](images/inference_scaling.png)
*图3: 传统推理与推理时扩展的对比分析*

---

## 3. 智能的具身化：多模态与 Agent 设计的未来

当模型具备了更强的推理能力和更灵活的思考方式后，智能的下一个前沿自然地指向了与物理世界更深度的交互。2025 年，**多模态** 和 **AI Agent** 的融合设计成为了 Google、微软等顶级研究机构的共同焦点。

**Google** 在其 2025 年的研究总结中，展示了从 **Gemini 3** 系列模型到 **Veo 3.1**（视频生成）、**Imagen 4**（图像生成）等一系列强大的多模态模型 [6]。更重要的是，他们将这些能力与 Agent 设计相结合，推出了如 **Jules**（异步编码代理）、**Google Antigravity**（AI 辅助软件开发平台）等工具，标志着 AI 从一个“辅助编码”的工具，向一个能够与开发者协作的“伙伴”转变。

**微软研究院** 则提出了更具前瞻性的概念——将 **“动作”（Action）视为首要模态** [7]。他们认为，构建一个能够统一处理文本、图像、声音和“动作”的生成式架构，是实现通用智能的根本性跃迁。在这一愿景下，AI 不再仅仅是世界的“观察者”，而是成为“参与者”。微软提出的 **空间智能（Spatial Intelligence）** 和 **世界模型（World Models）** 等概念，其目标正是让 AI Agent 能够理解物理世界的几何、物理和因果关系，通过“具身交互”在环境中学习和行动。

| 公司 | 核心模型/技术 | Agent 应用方向 | 核心理念 |
| :--- | :--- | :--- | :--- |
| **Google** | Gemini 3, Veo 3.1, Imagen 4 | Jules, Antigravity, NotebookLM | 将强大的多模态能力封装成协作式 Agent，提升生产力 |
| **微软** | （未明确模型） | AI for Science, Societal AI | 将“动作”作为首要模态，构建具身智能体，实现与物理世界的深度交互 |

这一趋势预示着，未来的 AI Agent 将不再局限于数字世界的信息处理，而是能够操作软件、控制机器人、进行科学实验，成为真正意义上的"数字员工"和"科研助手"。

![多模态智能体架构](images/multimodal_agent.png)
*图4: 多模态 AI Agent 的架构与应用方向*

---

## 结语：从“看热闹”到“学方法”

2025 年 AI 研究领域的这些底层突破，为从事科研与研发工作的读者提供了清晰的信号：

1.  **推理能力成为核心壁垒**：模型的价值不再仅由其知识广度决定，更取决于其逻辑推理的深度和可靠性。以 RLVR 为代表的训练方法，为提升特定领域模型的专业能力开辟了新路径。
2.  **智能成为一种可调度的资源**：通过推理时扩展技术，我们可以在成本和性能之间进行动态权衡。这意味着，针对关键的科研计算或分析任务，可以投入更多资源以换取更高的准确性。
3.  **Agent 设计是未来的方向**：AI 的应用正在从单一功能的工具，转向能够执行复杂任务、与环境交互的智能体。思考如何将 AI Agent 融入现有的研究和研发流程，将成为提升效率的关键。

对于真实参与研究工作的人而言，理解这些底层研究的演进方向，比追逐层出不穷的新模型、新应用更为重要。它帮助我们判断 AI 能力演进的真实推动力在哪里，并思考如何将这些正在浮现的新能力，转化为解决自身领域核心问题的强大助力。

---

### 参考文献

[1] Yin, Q., et al. (2025). *Evaluating Parameter Efficient Methods for RLVR*. arXiv.  
[2] Se, K., & Vert, A. (2025, December 10). *AI 101: The State of Reinforcement Learning in 2025*. Turing Post.  
[3] Burkov, A. (2025). *DeepSeek's R1 Breakthrough: AI Reasoning LLM*. LinkedIn.  
[4] Snell, C. V., et al. (2025). *Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning*. The Thirteenth International Conference on Learning Representations.  
[5] StepFun AI. (2025, December 9). *PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning*. GitHub.  
[6] Dean, J., Hassabis, D., & Manyika, J. (2025, December 23). *Google 2025 recap: Research breakthroughs of the year*. Google Blog.  
[7] Microsoft Research. (2025, December 12). *What’s next in AI?*. Microsoft Research Blog.  
